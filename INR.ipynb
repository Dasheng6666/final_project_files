{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import pakages"
      ],
      "metadata": {
        "id": "a3v74mZEqLOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import random\n",
        "import copy\n",
        "import piq  # PSNR SSIM LIPIS\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "UZhGt6vRqH3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seed setting, early stopping and data extraction"
      ],
      "metadata": {
        "id": "XiBBI_YDrVzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=2000, delta=0.000001, stop_threshold=0.000009):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.stop_threshold = stop_threshold\n",
        "\n",
        "    def __call__(self, current_loss):\n",
        "        if current_loss < self.stop_threshold:\n",
        "            print(f\"Loss reached threshold {self.stop_threshold}, early stopping\")\n",
        "            return True\n",
        "        if current_loss < self.best_loss - self.delta:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "            return False\n",
        "def extract_patch_data(patch_img, num_freq=10):\n",
        "    patch_np = np.array(patch_img)\n",
        "    h, w, _ = patch_np.shape\n",
        "    coords = []\n",
        "    for y in range(h):\n",
        "        for x in range(w):\n",
        "            # normalization\n",
        "            x_norm = (x / (w-1)) * 2 - 1\n",
        "            y_norm = (y / (h-1)) * 2 - 1\n",
        "            coords.append([x_norm, y_norm])\n",
        "    return np.array(coords), patch_np.reshape(-1, 3)/255.0, h, w\n",
        "\n",
        "def generate_patch(model, device, actual_size, num_freq=10):\n",
        "    h, w = actual_size\n",
        "    coords = []\n",
        "    for y in range(h):\n",
        "        for x in range(w):\n",
        "            x_norm = (x / (w-1)) * 2 - 1\n",
        "            y_norm = (y / (h-1)) * 2 - 1\n",
        "            coords.append([x_norm, y_norm])\n",
        "\n",
        "    coords_tensor = torch.tensor(coords, dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        rgb = model(coords_tensor).cpu().numpy()\n",
        "\n",
        "    return (np.clip(rgb, 0, 1)*255).astype(np.uint8).reshape(h, w, 3)"
      ],
      "metadata": {
        "id": "XN--vqnCrZ8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP"
      ],
      "metadata": {
        "id": "rATbwHXApoWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(x, y, num_frequencies=10):\n",
        "    frequencies = 2 ** torch.arange(num_frequencies, dtype=torch.float32)\n",
        "    x_enc = torch.cat([torch.sin(frequencies * x), torch.cos(frequencies * x)])\n",
        "    y_enc = torch.cat([torch.sin(frequencies * y), torch.cos(frequencies * y)])\n",
        "    return torch.cat([x_enc, y_enc])\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim=40, hidden_dims=[32]*4+[16], output_dim=3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dims = [input_dim] + hidden_dims + [output_dim]\n",
        "        for i in range(len(dims)-1):\n",
        "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "            if i < len(dims)-2:\n",
        "                layers.append(nn.ReLU())\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.net(x))"
      ],
      "metadata": {
        "id": "G7vZKGzCprt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SIREN\n"
      ],
      "metadata": {
        "id": "0AE2BaO5pwj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SineLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, is_first=False, omega_0=30.0):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "        # 修正初始化策略\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                bound = 1 / in_features\n",
        "            else:\n",
        "                bound = np.sqrt(6 / in_features) / omega_0\n",
        "            self.linear.weight.uniform_(-bound, bound)\n",
        "            if self.linear.bias is not None:\n",
        "                self.linear.bias.uniform_(-bound, bound)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.sin(self.omega_0 * self.linear(input))\n",
        "\n",
        "class Siren(nn.Module):\n",
        "    def __init__(self, in_features=2, hidden_features=32, hidden_layers=5, out_features=3):\n",
        "        super().__init__()\n",
        "        self.net = nn.ModuleList()\n",
        "\n",
        "        # first\n",
        "        self.net.append(SineLayer(in_features, hidden_features, is_first=True))\n",
        "\n",
        "        # hidden\n",
        "        for _ in range(hidden_layers-2):\n",
        "            self.net.append(SineLayer(hidden_features, hidden_features))\n",
        "        self.net.append(SineLayer(hidden_features, 16))\n",
        "\n",
        "        # output\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.Linear(16, out_features),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # output initial\n",
        "        with torch.no_grad():\n",
        "            self.final_layer[0].weight.uniform_(-np.sqrt(6/hidden_features),\n",
        "                                             np.sqrt(6/hidden_features))\n",
        "\n",
        "    def forward(self, coords):\n",
        "        x = coords\n",
        "        for layer in self.net:\n",
        "            x = layer(x)\n",
        "        return self.final_layer(x)"
      ],
      "metadata": {
        "id": "6Z7XKckKpx4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FilmSIREN"
      ],
      "metadata": {
        "id": "iLMO2QBZpr6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def frequency_init(freq):\n",
        "    def init(m):\n",
        "        with torch.no_grad():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                num_input = m.weight.size(-1)\n",
        "                m.weight.uniform_(-np.sqrt(6 / num_input) / freq, np.sqrt(6 / num_input) / freq)\n",
        "    return init\n",
        "\n",
        "\n",
        "class FiLMLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x, freq, phase_shift):\n",
        "        x = self.layer(x)\n",
        "        return torch.sin(freq * x + phase_shift)\n",
        "\n",
        "class CustomMappingNetwork(nn.Module):\n",
        "    def __init__(self, z_dim, map_hidden_dim, map_output_dim):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(z_dim, map_hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(map_hidden_dim, map_hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(map_hidden_dim, map_hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(map_hidden_dim, map_output_dim)\n",
        "        )\n",
        "        self.network.apply(frequency_init(25))\n",
        "        with torch.no_grad():\n",
        "            self.network[-1].weight *= 0.25\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.network(z)\n",
        "        return out[..., :out.shape[-1]//2], out[..., out.shape[-1]//2:]\n",
        "\n",
        "class FilmSIREN(nn.Module):\n",
        "    def __init__(self, input_dim=2, z_dim=64, hidden_dim=36, output_dim=3, device=None):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.z = nn.Parameter(torch.randn(1, z_dim))\n",
        "\n",
        "        self.network = nn.ModuleList([\n",
        "            FiLMLayer(input_dim, hidden_dim),\n",
        "            FiLMLayer(hidden_dim, hidden_dim),\n",
        "            FiLMLayer(hidden_dim, hidden_dim),\n",
        "        ])\n",
        "        self.final_layer = nn.Linear(hidden_dim, output_dim)\n",
        "        self.mapping_network = CustomMappingNetwork(z_dim, 6, len(self.network)*hidden_dim*2)\n",
        "\n",
        "        self.network.apply(frequency_init(25))\n",
        "        self.final_layer.apply(frequency_init(25))\n",
        "        self.network[0].layer.apply(lambda m: m.weight.data.uniform_(-1./input_dim, 1./input_dim))\n",
        "\n",
        "    def forward(self, coords):\n",
        "        frequencies, phase_shifts = self.mapping_network(self.z)\n",
        "        frequencies = frequencies * 15 + 30\n",
        "\n",
        "        x = coords\n",
        "        for idx, layer in enumerate(self.network):\n",
        "            start = idx * self.network[0].layer.out_features\n",
        "            end = (idx+1) * self.network[0].layer.out_features\n",
        "            x = layer(x, frequencies[:, start:end], phase_shifts[:, start:end])\n",
        "\n",
        "        return torch.sigmoid(self.final_layer(x))"
      ],
      "metadata": {
        "id": "ctYG0Govpz6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_siren(model, coords, rgb_values, device, total_steps=10000):\n",
        "    coords_tensor = torch.tensor(coords, dtype=torch.float32).to(device)\n",
        "    rgb_tensor = torch.tensor(rgb_values, dtype=torch.float32).to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, 5000, 0.5)\n",
        "    early_stop = EarlyStopping(patience=4000)\n",
        "\n",
        "    for step in range(total_steps):\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(coords_tensor)\n",
        "        loss = criterion(pred, rgb_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if early_stop(loss.item()):\n",
        "            print(f\"Early stopping at step {step}\")\n",
        "            break\n",
        "\n",
        "        if step % 2000 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.item():.6f}\")\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    set_seed(42)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Parameters\n",
        "    image_path = cropped_image_path\n",
        "    patch_size = 64\n",
        "    overlap = patch_size // 4\n",
        "    stride = patch_size - overlap\n",
        "\n",
        "    # Load image\n",
        "    full_img = Image.open(image_path).convert(\"RGB\")\n",
        "    orig_w, orig_h = full_img.size\n",
        "    original_np = np.array(full_img)\n",
        "\n",
        "    # Initialization for reconstruction canvas\n",
        "    sum_canvas = np.zeros((orig_h, orig_w, 3), dtype=np.float32)\n",
        "    count_canvas = np.zeros((orig_h, orig_w, 1), dtype=np.float32)\n",
        "\n",
        "    # Metrics storage\n",
        "    psnr_values = []\n",
        "    ssim_values = []\n",
        "    patch_count = 0\n",
        "\n",
        "    # Create a directory to save the weights\n",
        "    weights_dir = \"saved_siren_016_weights\"\n",
        "    os.makedirs(weights_dir, exist_ok=True)\n",
        "\n",
        "    # Process each patch\n",
        "    for i in range(0, orig_w, stride):\n",
        "        for j in range(0, orig_h, stride):\n",
        "            left = max(0, min(i, orig_w - patch_size))\n",
        "            upper = max(0, min(j, orig_h - patch_size))\n",
        "            right = left + patch_size\n",
        "            lower = upper + patch_size\n",
        "\n",
        "            patch = full_img.crop((left, upper, right, lower))\n",
        "            coords, rgb, h_patch, w_patch = extract_patch_data(patch)\n",
        "\n",
        "            # Initialize the model\n",
        "            model = Siren(in_features=2, hidden_features=32, hidden_layers=6).to(device)\n",
        "            print(f\"\\nTraining patch at ({left},{upper})\")\n",
        "            trained_model = train_siren(model, coords, rgb, device)\n",
        "\n",
        "            # summary\n",
        "            if i==0 and j==0:\n",
        "                summary(model, (2,))\n",
        "\n",
        "\n",
        "            # Save weights after training each patch\n",
        "            patch_weight_path = os.path.join(weights_dir, f'patch_{patch_count}.pth')\n",
        "            torch.save(trained_model.state_dict(), patch_weight_path)\n",
        "\n",
        "            # Generate results\n",
        "            with torch.no_grad():\n",
        "                coords_tensor = torch.tensor(coords, dtype=torch.float32).to(device)\n",
        "                output = trained_model(coords_tensor).cpu().numpy()\n",
        "\n",
        "\n",
        "            generated = (output * 255).clip(0, 255).astype(np.uint8).reshape(h_patch, w_patch, 3)\n",
        "            sum_canvas[upper:lower, left:right] += generated\n",
        "            count_canvas[upper:lower, left:right] += 1\n",
        "\n",
        "\n",
        "    # Generate final image\n",
        "    count_canvas[count_canvas == 0] = 1  # Avoid division by zero\n",
        "    final_image = (sum_canvas / count_canvas).astype(np.uint8)\n",
        "    Image.fromarray(final_image).save(\"full_reconstruction_siren_016_64.png\")\n"
      ],
      "metadata": {
        "id": "33pGiomRsCC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_siren(model, coords, rgb_values, device, total_steps=10000):\n",
        "    coords_tensor = torch.tensor(coords, dtype=torch.float32).to(device)\n",
        "    rgb_tensor = torch.tensor(rgb_values, dtype=torch.float32).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.5)\n",
        "    criterion = nn.MSELoss()\n",
        "    early_stop = EarlyStopping(patience=4000)\n",
        "\n",
        "    for step in range(total_steps):\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(coords_tensor)\n",
        "        loss = criterion(pred, rgb_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if early_stop(loss.item()):\n",
        "            print(f\"Early stopping at step {step}\")\n",
        "            break\n",
        "\n",
        "        if step % 2000 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    set_seed(42)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "    sum_canvas = np.zeros((orig_h, orig_w, 3), dtype=np.float32)\n",
        "    count_canvas = np.zeros((orig_h, orig_w, 1), dtype=np.float32)\n",
        "\n",
        "    # each patch\n",
        "    psnr_values = []\n",
        "    ssim_values = []\n",
        "    prev_state_dict = None\n",
        "    patch_count = 0\n",
        "    weights_dir = \"saved_filmsiren_016_weights\"\n",
        "    os.makedirs(weights_dir, exist_ok=True)\n",
        "\n",
        "    for i in range(0, orig_w, stride):\n",
        "        for j in range(0, orig_h, stride):\n",
        "\n",
        "            left = max(0, min(i, orig_w - patch_size))\n",
        "            upper = max(0, min(j, orig_h - patch_size))\n",
        "            right = left + patch_size\n",
        "            lower = upper + patch_size\n",
        "\n",
        "            # current patch\n",
        "            patch = full_img.crop((left, upper, right, lower))\n",
        "            actual_h, actual_w = patch.size[1], patch.size[0]\n",
        "\n",
        "            # data\n",
        "            coords, rgb, h_patch, w_patch = extract_patch_data(patch)\n",
        "\n",
        "            # train（from previous）\n",
        "            model = FilmSIREN(device=device).to(device)\n",
        "            if prev_state_dict is not None:\n",
        "                model.load_state_dict(prev_state_dict, strict=False)\n",
        "            if j == 0 and i == 0 :\n",
        "                summary(model, input_size=(2,), device=device.type)\n",
        "            # current patch\n",
        "            print(f\"\\nTraining patch at ({left},{upper})\")\n",
        "            trained_model = train_siren(model, coords, rgb, device)\n",
        "            patch_weight_path = os.path.join(weights_dir, f'patch_{patch_count}.pth')\n",
        "            torch.save(trained_model.state_dict(), patch_weight_path)\n",
        "            # custom weight（except z）\n",
        "            current_state_dict = copy.deepcopy(trained_model.state_dict())\n",
        "            del current_state_dict['z']\n",
        "            prev_state_dict = current_state_dict\n",
        "\n",
        "            # output\n",
        "            with torch.no_grad():\n",
        "                coords_tensor = torch.tensor(coords, dtype=torch.float32).to(device)\n",
        "                output = trained_model(coords_tensor).cpu().numpy()\n",
        "\n",
        "\n",
        "            generated = (output * 255).clip(0, 255).astype(np.float32)\n",
        "            generated = generated.reshape(actual_h, actual_w, 3)\n",
        "\n",
        "            # final\n",
        "            sum_canvas[upper:lower, left:right] += generated\n",
        "            count_canvas[upper:lower, left:right] += 1\n",
        "\n",
        "\n",
        "\n",
        "    # final\n",
        "    count_canvas[count_canvas == 0] = 1\n",
        "    final_image = (sum_canvas / count_canvas).astype(np.uint8)\n",
        "    Image.fromarray(final_image).save(\"full_reconstruction_filsiren_016_64.png\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cZ-G-qnmt4QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "recon"
      ],
      "metadata": {
        "id": "VaP0zXN42Iw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# def enlarge_image(input_path, output_path, new_size=(4094, 2400)):\n",
        "def enlarge_image(input_path, output_path, new_size=(1920, 1080)):\n",
        "\n",
        "    original_image = Image.open(input_path)\n",
        "    original_width, original_height = original_image.size\n",
        "    new_image = Image.new(\"RGB\", new_size, (255, 255, 255))\n",
        "\n",
        "    # central\n",
        "    top_left_x = (new_size[0] - original_width) // 2\n",
        "    top_left_y = (new_size[1] - original_height) // 2\n",
        "\n",
        "    new_image.paste(original_image, (top_left_x, top_left_y))\n",
        "    new_image.save(output_path)\n",
        "\n",
        "\n",
        "input_image_path = path  # image path\n",
        "\n",
        "output_image_path = 'enlarged_image.png'  # larger image\n",
        "enlarge_image(input_image_path, output_image_path)\n",
        "width1, height1 = image.size\n",
        "\n",
        "\n",
        "print(f\"Image Size: {width1} x {height1}\")"
      ],
      "metadata": {
        "id": "7iMaaiNk2KzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import odak\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from argparse import Namespace\n",
        "\n",
        "args_prop = Namespace(\n",
        "    wavelengths=[639e-9, 515e-9, 473e-9],\n",
        "    pixel_pitch=3.74e-6,\n",
        "    volume_depth=5e-3,\n",
        "    d_val=0.,\n",
        "    pad_size=[1080, 1920],\n",
        "    aperture_size = 1500,\n",
        "    device = \"cuda\"\n",
        ")\n",
        "\n",
        "\n",
        "propagator = odak.learn.wave.propagator(\n",
        "    resolution = args_prop.pad_size,\n",
        "    wavelengths = args_prop.wavelengths,\n",
        "    pixel_pitch = args_prop.pixel_pitch,\n",
        "    number_of_frames = 3,\n",
        "    number_of_depth_layers = 3,\n",
        "    volume_depth = args_prop.volume_depth,\n",
        "    image_location_offset = args_prop.d_val,\n",
        "    propagation_type = 'Bandlimited Angular Spectrum',\n",
        "    propagator_type = 'forward',\n",
        "    laser_channel_power = torch.eye(3),\n",
        "    aperture_size = args_prop.aperture_size,\n",
        "    aperture = None,\n",
        "    method = 'conventional',\n",
        "    device = args_prop.device\n",
        ")\n",
        "phase_map = odak.learn.tools.load_image(r\"enlarged_image.png\", normalizeby = 255., torch_style = True).to(args_prop.device)\n",
        "phase_map = (phase_map * 2 * odak.pi) % (2 * odak.pi)\n",
        "\n",
        "try:\n",
        "    recon_output = propagator.reconstruct(phase_map, amplitude=None, no_grad = True)\n",
        "except Exception as e:\n",
        "    print(\"Error during reconstruction:\", e)\n",
        "\n",
        "print(recon_output.size())\n",
        "reconstruction_intensities = torch.sum(recon_output, dim = 0)\n",
        "for idx, recon in enumerate(reconstruction_intensities):\n",
        "    odak.learn.tools.save_image(\n",
        "        f\"./recon_{idx}.png\",\n",
        "        recon,\n",
        "        cmin=0.,\n",
        "        cmax=1.0\n",
        "    )"
      ],
      "metadata": {
        "id": "uuBvjnCZ2d3E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}